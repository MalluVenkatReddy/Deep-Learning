{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "695d0a80",
   "metadata": {},
   "source": [
    "## Objective: The objective of this assignment is to assess students' understanding of batch normalization in artificial neural networks (ANN) and its impact on training performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19639894",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Theory and Concepts\n",
    "1. Explain the concept of batch normalization in the context of Artificial Neural Networks\n",
    "2. Describe the benefits of using batch normalization during training\n",
    "3. Discuss the working principle of batch normalization, including the normalization step and the learnable \n",
    "parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650c6875",
   "metadata": {},
   "source": [
    "1. Batch Norm is a normalization technique done between the layers of a Neural Network instead of in the raw data. It is done along mini-batches instead of the full data set. It serves to speed up training and use higher learning rates, making learning easier.\n",
    "Batch-Normalization (BN) is an algorithmic method which makes the training of Deep Neural Networks (DNN) faster and more stable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45335d33",
   "metadata": {},
   "source": [
    "2. Using batch normalization allows us to use much higher learning rates, which further increases the speed at which networks train. Makes weights easier to initialize — Weight initialization can be difficult, and it's even more difficult when creating deeper networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615cdb56",
   "metadata": {},
   "source": [
    "3. It consists of normalizing activation vectors from hidden layers using the first and the second statistical moments (mean and variance) of the current batch. This normalization step is applied right before (or right after) the nonlinear function.\n",
    "\n",
    "\n",
    "Batch Norm is just another network layer that gets inserted between a hidden layer and the next hidden layer. Its job is to take the outputs from the first hidden layer and normalize them before passing them on as the input of the next hidden layer.\n",
    "\n",
    "Just like the parameters (eg. weights, bias) of any network layer, a Batch Norm layer also has parameters of its own:\n",
    "    \n",
    "Two learnable parameters called beta and gamma.\n",
    "Two non-learnable parameters (Mean Moving Average and Variance Moving Average) are saved as part of the ‘state’ of the Batch Norm layer.\n",
    "\n",
    "\n",
    "These parameters are per Batch Norm layer. So if we have, say, three hidden layers and three Batch Norm layers in the network, we would have three learnable beta and gamma parameters for the three layers. Similarly for the Moving Average parameters.\n",
    "\n",
    "During training, we feed the network one mini-batch of data at a time. During the forward pass, each layer of the network processes that mini-batch of data. The Batch Norm layer processes its data as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7c0cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Implementation\n",
    "1. Choose a dataset of your choice (e.g., MNIST, CIFAR-10) and preprocess it\n",
    "2. Implement a simple feedforward neural network using any deep learning framework/library (e.g., \n",
    "TensorFlow, PyTorch)\n",
    "3. Train the neural network on the chosen dataset without using batch normalization\n",
    "4. Implement batch normalization layers in the neural network and train the model again\n",
    "5. Compare the training and validation performance (e.g., accuracy, loss) between the models with and \n",
    "without batch normalizationr\n",
    "6 Discuss the impact of batch normalization on the training process and the performance of the neural \n",
    "network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd11c97",
   "metadata": {},
   "source": [
    "## With batch normalization (size 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "533b3e56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.3395 - accuracy: 0.9012 - val_loss: 0.1523 - val_accuracy: 0.9570\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1688 - accuracy: 0.9514 - val_loss: 0.1128 - val_accuracy: 0.9658\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1247 - accuracy: 0.9639 - val_loss: 0.0992 - val_accuracy: 0.9704\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0987 - accuracy: 0.9714 - val_loss: 0.0923 - val_accuracy: 0.9702\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0821 - accuracy: 0.9761 - val_loss: 0.0851 - val_accuracy: 0.9740\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0671 - accuracy: 0.9806 - val_loss: 0.0818 - val_accuracy: 0.9738\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0582 - accuracy: 0.9832 - val_loss: 0.0816 - val_accuracy: 0.9768\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0499 - accuracy: 0.9857 - val_loss: 0.0771 - val_accuracy: 0.9768\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0420 - accuracy: 0.9885 - val_loss: 0.0789 - val_accuracy: 0.9750\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0380 - accuracy: 0.9893 - val_loss: 0.0779 - val_accuracy: 0.9772\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.339461</td>\n",
       "      <td>0.901236</td>\n",
       "      <td>0.152341</td>\n",
       "      <td>0.9570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.168787</td>\n",
       "      <td>0.951400</td>\n",
       "      <td>0.112844</td>\n",
       "      <td>0.9658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.124734</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.099177</td>\n",
       "      <td>0.9704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.098705</td>\n",
       "      <td>0.971364</td>\n",
       "      <td>0.092308</td>\n",
       "      <td>0.9702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.082098</td>\n",
       "      <td>0.976055</td>\n",
       "      <td>0.085137</td>\n",
       "      <td>0.9740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.067097</td>\n",
       "      <td>0.980600</td>\n",
       "      <td>0.081810</td>\n",
       "      <td>0.9738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.058196</td>\n",
       "      <td>0.983182</td>\n",
       "      <td>0.081604</td>\n",
       "      <td>0.9768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.049940</td>\n",
       "      <td>0.985727</td>\n",
       "      <td>0.077088</td>\n",
       "      <td>0.9768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.042007</td>\n",
       "      <td>0.988509</td>\n",
       "      <td>0.078856</td>\n",
       "      <td>0.9750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.038037</td>\n",
       "      <td>0.989345</td>\n",
       "      <td>0.077891</td>\n",
       "      <td>0.9772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  val_loss  val_accuracy\n",
       "0  0.339461  0.901236  0.152341        0.9570\n",
       "1  0.168787  0.951400  0.112844        0.9658\n",
       "2  0.124734  0.963855  0.099177        0.9704\n",
       "3  0.098705  0.971364  0.092308        0.9702\n",
       "4  0.082098  0.976055  0.085137        0.9740\n",
       "5  0.067097  0.980600  0.081810        0.9738\n",
       "6  0.058196  0.983182  0.081604        0.9768\n",
       "7  0.049940  0.985727  0.077088        0.9768\n",
       "8  0.042007  0.988509  0.078856        0.9750\n",
       "9  0.038037  0.989345  0.077891        0.9772"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "mnist=tf.keras.datasets.mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_valid,X_train=X_train_full[:5000]/255., X_train_full[5000:]/255.\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# scale the test set as well\n",
    "X_test = X_test / 255.\n",
    "\n",
    "# Creating layers of ANN\n",
    "LAYERS=[tf.keras.layers.Flatten(input_shape=[28, 28], name=\"inputLayer\"),\n",
    "        tf.keras.layers.Dense(300, activation=\"relu\", name=\"hiddenLayer1\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(100, activation=\"relu\", name=\"hiddenLayer2\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\", name=\"outputLayer\")]\n",
    "\n",
    "model_clf=tf.keras.models.Sequential(LAYERS)\n",
    "\n",
    "LOSS_FUNCTION = \"sparse_categorical_crossentropy\" # use => tf.losses.sparse_categorical_crossentropy\n",
    "OPTIMIZER = \"SGD\" # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)\n",
    "METRICS = [\"accuracy\"]\n",
    "\n",
    "\n",
    "model_clf.compile(loss=LOSS_FUNCTION,\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=METRICS)\n",
    "\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "VALIDATION_SET = (X_valid, y_valid)\n",
    "\n",
    "history=model_clf.fit(X_train,y_train,epochs=EPOCHS,validation_data=VALIDATION_SET,batch_size=32)\n",
    "\n",
    "pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af48f6a",
   "metadata": {},
   "source": [
    "## Without batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5112a009",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "mnist=tf.keras.datasets.mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_valid,X_train=X_train_full[:5000]/255., X_train_full[5000:]/255.\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# scale the test set as well\n",
    "X_test = X_test / 255.\n",
    "\n",
    "# Creating layers of ANN\n",
    "LAYERS=[tf.keras.layers.Flatten(input_shape=[28, 28], name=\"inputLayer\"),\n",
    "        tf.keras.layers.Dense(300, activation=\"relu\", name=\"hiddenLayer1\"),\n",
    "        tf.keras.layers.Dense(100, activation=\"relu\", name=\"hiddenLayer2\"),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\", name=\"outputLayer\")]\n",
    "\n",
    "model_clf=tf.keras.models.Sequential(LAYERS)\n",
    "\n",
    "LOSS_FUNCTION = \"sparse_categorical_crossentropy\" # use => tf.losses.sparse_categorical_crossentropy\n",
    "OPTIMIZER = \"SGD\" # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)\n",
    "METRICS = [\"accuracy\"]\n",
    "\n",
    "\n",
    "model_clf.compile(loss=LOSS_FUNCTION,\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=METRICS)\n",
    "\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "VALIDATION_SET = (X_valid, y_valid)\n",
    "\n",
    "history=model_clf.fit(X_train,y_train,epochs=EPOCHS,validation_data=VALIDATION_SET,batch_size=32)\n",
    "\n",
    "pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5c5152",
   "metadata": {},
   "source": [
    "## accuracy is increased with batch normalization from 0.969 to 0.989 (for training data) and 0.967 to 0.977 (for validation data)\n",
    "\n",
    "## loss is reduced with batch normalization from 0.108 to 0.038 (for training data) and 0.111 to 0.077 (for validation data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfc0408",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Experimentation and Analysis\n",
    "1. Experiment with different batch sizes and observe the effect on the training dynamics and model \n",
    "performancer\n",
    "2. Discuss the advantages and potential limitations of batch normalization in improving the training of \n",
    "neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f752e0",
   "metadata": {},
   "source": [
    "## With batch normalization (size 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28ab96a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "860/860 [==============================] - 6s 5ms/step - loss: 0.4000 - accuracy: 0.8829 - val_loss: 0.2059 - val_accuracy: 0.9436\n",
      "Epoch 2/10\n",
      "860/860 [==============================] - 3s 4ms/step - loss: 0.1941 - accuracy: 0.9463 - val_loss: 0.1555 - val_accuracy: 0.9548\n",
      "Epoch 3/10\n",
      "860/860 [==============================] - 3s 3ms/step - loss: 0.1476 - accuracy: 0.9578 - val_loss: 0.1302 - val_accuracy: 0.9616\n",
      "Epoch 4/10\n",
      "860/860 [==============================] - 3s 3ms/step - loss: 0.1184 - accuracy: 0.9664 - val_loss: 0.1167 - val_accuracy: 0.9644\n",
      "Epoch 5/10\n",
      "860/860 [==============================] - 3s 3ms/step - loss: 0.0999 - accuracy: 0.9719 - val_loss: 0.1064 - val_accuracy: 0.9680\n",
      "Epoch 6/10\n",
      "860/860 [==============================] - 3s 3ms/step - loss: 0.0847 - accuracy: 0.9765 - val_loss: 0.0994 - val_accuracy: 0.9686\n",
      "Epoch 7/10\n",
      "860/860 [==============================] - 3s 3ms/step - loss: 0.0732 - accuracy: 0.9802 - val_loss: 0.0945 - val_accuracy: 0.9710\n",
      "Epoch 8/10\n",
      "860/860 [==============================] - 3s 3ms/step - loss: 0.0643 - accuracy: 0.9833 - val_loss: 0.0916 - val_accuracy: 0.9710\n",
      "Epoch 9/10\n",
      "860/860 [==============================] - 3s 3ms/step - loss: 0.0563 - accuracy: 0.9853 - val_loss: 0.0873 - val_accuracy: 0.9734\n",
      "Epoch 10/10\n",
      "860/860 [==============================] - 3s 3ms/step - loss: 0.0497 - accuracy: 0.9869 - val_loss: 0.0863 - val_accuracy: 0.9740\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.400043</td>\n",
       "      <td>0.882927</td>\n",
       "      <td>0.205926</td>\n",
       "      <td>0.9436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.194065</td>\n",
       "      <td>0.946345</td>\n",
       "      <td>0.155528</td>\n",
       "      <td>0.9548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.147613</td>\n",
       "      <td>0.957800</td>\n",
       "      <td>0.130193</td>\n",
       "      <td>0.9616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.118389</td>\n",
       "      <td>0.966382</td>\n",
       "      <td>0.116655</td>\n",
       "      <td>0.9644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.099855</td>\n",
       "      <td>0.971891</td>\n",
       "      <td>0.106387</td>\n",
       "      <td>0.9680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.084659</td>\n",
       "      <td>0.976491</td>\n",
       "      <td>0.099388</td>\n",
       "      <td>0.9686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.073190</td>\n",
       "      <td>0.980182</td>\n",
       "      <td>0.094548</td>\n",
       "      <td>0.9710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.064344</td>\n",
       "      <td>0.983345</td>\n",
       "      <td>0.091589</td>\n",
       "      <td>0.9710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.056262</td>\n",
       "      <td>0.985327</td>\n",
       "      <td>0.087270</td>\n",
       "      <td>0.9734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.049713</td>\n",
       "      <td>0.986945</td>\n",
       "      <td>0.086281</td>\n",
       "      <td>0.9740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  val_loss  val_accuracy\n",
       "0  0.400043  0.882927  0.205926        0.9436\n",
       "1  0.194065  0.946345  0.155528        0.9548\n",
       "2  0.147613  0.957800  0.130193        0.9616\n",
       "3  0.118389  0.966382  0.116655        0.9644\n",
       "4  0.099855  0.971891  0.106387        0.9680\n",
       "5  0.084659  0.976491  0.099388        0.9686\n",
       "6  0.073190  0.980182  0.094548        0.9710\n",
       "7  0.064344  0.983345  0.091589        0.9710\n",
       "8  0.056262  0.985327  0.087270        0.9734\n",
       "9  0.049713  0.986945  0.086281        0.9740"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "mnist=tf.keras.datasets.mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_valid,X_train=X_train_full[:5000]/255., X_train_full[5000:]/255.\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# scale the test set as well\n",
    "X_test = X_test / 255.\n",
    "\n",
    "# Creating layers of ANN\n",
    "LAYERS=[tf.keras.layers.Flatten(input_shape=[28, 28], name=\"inputLayer\"),\n",
    "        tf.keras.layers.Dense(300, activation=\"relu\", name=\"hiddenLayer1\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(100, activation=\"relu\", name=\"hiddenLayer2\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\", name=\"outputLayer\")]\n",
    "\n",
    "model_clf=tf.keras.models.Sequential(LAYERS)\n",
    "\n",
    "LOSS_FUNCTION = \"sparse_categorical_crossentropy\" # use => tf.losses.sparse_categorical_crossentropy\n",
    "OPTIMIZER = \"SGD\" # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)\n",
    "METRICS = [\"accuracy\"]\n",
    "\n",
    "\n",
    "model_clf.compile(loss=LOSS_FUNCTION,\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=METRICS)\n",
    "\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "VALIDATION_SET = (X_valid, y_valid)\n",
    "\n",
    "history=model_clf.fit(X_train,y_train,epochs=EPOCHS,validation_data=VALIDATION_SET,batch_size=64)\n",
    "\n",
    "pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bd0b34",
   "metadata": {},
   "source": [
    "## With batch normalization (size 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cb80945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3438/3438 [==============================] - 13s 3ms/step - loss: 0.3112 - accuracy: 0.9062 - val_loss: 0.1335 - val_accuracy: 0.9640\n",
      "Epoch 2/10\n",
      "3438/3438 [==============================] - 9s 3ms/step - loss: 0.1633 - accuracy: 0.9514 - val_loss: 0.1062 - val_accuracy: 0.9696\n",
      "Epoch 3/10\n",
      "3438/3438 [==============================] - 9s 3ms/step - loss: 0.1246 - accuracy: 0.9627 - val_loss: 0.0858 - val_accuracy: 0.9750\n",
      "Epoch 4/10\n",
      "3438/3438 [==============================] - 9s 3ms/step - loss: 0.1008 - accuracy: 0.9690 - val_loss: 0.0774 - val_accuracy: 0.9780\n",
      "Epoch 5/10\n",
      "3438/3438 [==============================] - 9s 3ms/step - loss: 0.0860 - accuracy: 0.9740 - val_loss: 0.0746 - val_accuracy: 0.9794\n",
      "Epoch 6/10\n",
      "3438/3438 [==============================] - 9s 3ms/step - loss: 0.0763 - accuracy: 0.9766 - val_loss: 0.0696 - val_accuracy: 0.9814\n",
      "Epoch 7/10\n",
      "3438/3438 [==============================] - 9s 3ms/step - loss: 0.0663 - accuracy: 0.9795 - val_loss: 0.0640 - val_accuracy: 0.9812\n",
      "Epoch 8/10\n",
      "3438/3438 [==============================] - 10s 3ms/step - loss: 0.0605 - accuracy: 0.9811 - val_loss: 0.0667 - val_accuracy: 0.9820\n",
      "Epoch 9/10\n",
      "3438/3438 [==============================] - 10s 3ms/step - loss: 0.0510 - accuracy: 0.9838 - val_loss: 0.0691 - val_accuracy: 0.9816\n",
      "Epoch 10/10\n",
      "3438/3438 [==============================] - 10s 3ms/step - loss: 0.0479 - accuracy: 0.9849 - val_loss: 0.0724 - val_accuracy: 0.9794\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.311245</td>\n",
       "      <td>0.906182</td>\n",
       "      <td>0.133507</td>\n",
       "      <td>0.9640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.163277</td>\n",
       "      <td>0.951382</td>\n",
       "      <td>0.106187</td>\n",
       "      <td>0.9696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.124562</td>\n",
       "      <td>0.962727</td>\n",
       "      <td>0.085798</td>\n",
       "      <td>0.9750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.100842</td>\n",
       "      <td>0.969000</td>\n",
       "      <td>0.077413</td>\n",
       "      <td>0.9780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.086010</td>\n",
       "      <td>0.973964</td>\n",
       "      <td>0.074630</td>\n",
       "      <td>0.9794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.076302</td>\n",
       "      <td>0.976636</td>\n",
       "      <td>0.069615</td>\n",
       "      <td>0.9814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.066287</td>\n",
       "      <td>0.979545</td>\n",
       "      <td>0.064046</td>\n",
       "      <td>0.9812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.060520</td>\n",
       "      <td>0.981127</td>\n",
       "      <td>0.066720</td>\n",
       "      <td>0.9820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.051045</td>\n",
       "      <td>0.983764</td>\n",
       "      <td>0.069149</td>\n",
       "      <td>0.9816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.047922</td>\n",
       "      <td>0.984927</td>\n",
       "      <td>0.072400</td>\n",
       "      <td>0.9794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  val_loss  val_accuracy\n",
       "0  0.311245  0.906182  0.133507        0.9640\n",
       "1  0.163277  0.951382  0.106187        0.9696\n",
       "2  0.124562  0.962727  0.085798        0.9750\n",
       "3  0.100842  0.969000  0.077413        0.9780\n",
       "4  0.086010  0.973964  0.074630        0.9794\n",
       "5  0.076302  0.976636  0.069615        0.9814\n",
       "6  0.066287  0.979545  0.064046        0.9812\n",
       "7  0.060520  0.981127  0.066720        0.9820\n",
       "8  0.051045  0.983764  0.069149        0.9816\n",
       "9  0.047922  0.984927  0.072400        0.9794"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "mnist=tf.keras.datasets.mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_valid,X_train=X_train_full[:5000]/255., X_train_full[5000:]/255.\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# scale the test set as well\n",
    "X_test = X_test / 255.\n",
    "\n",
    "# Creating layers of ANN\n",
    "LAYERS=[tf.keras.layers.Flatten(input_shape=[28, 28], name=\"inputLayer\"),\n",
    "        tf.keras.layers.Dense(300, activation=\"relu\", name=\"hiddenLayer1\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(100, activation=\"relu\", name=\"hiddenLayer2\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\", name=\"outputLayer\")]\n",
    "\n",
    "model_clf=tf.keras.models.Sequential(LAYERS)\n",
    "\n",
    "LOSS_FUNCTION = \"sparse_categorical_crossentropy\" # use => tf.losses.sparse_categorical_crossentropy\n",
    "OPTIMIZER = \"SGD\" # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)\n",
    "METRICS = [\"accuracy\"]\n",
    "\n",
    "\n",
    "model_clf.compile(loss=LOSS_FUNCTION,\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=METRICS)\n",
    "\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "VALIDATION_SET = (X_valid, y_valid)\n",
    "\n",
    "history=model_clf.fit(X_train,y_train,epochs=EPOCHS,validation_data=VALIDATION_SET,batch_size=16)\n",
    "\n",
    "pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9795c116",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "    1. batch size 32 get better results than batch size 16 and 32, \n",
    "    2. so we need to use batch size as hyper parameter to get the optimum batch size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
