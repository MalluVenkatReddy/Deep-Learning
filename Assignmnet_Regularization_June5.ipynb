{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0de89cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Part 1: Understanding Regularization\n",
    "1. What is regularization in the context of deep learning? Why is it important\n",
    "2. Explain the bias-variance tradeoff and how regularization helps in addressing this tradeoff\n",
    "3. Describe the concept of L1 and L2 regularization. How do they differ in terms of penalty calculation and \n",
    "their effects on the model\n",
    "4. Discuss the role of regularization in preventing overfitting and improving the generalization of deep \n",
    "learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a568d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What is regularization in the context of deep learning? Why is it important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31867faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularization is a set of techniques that can prevent overfitting in neural networks and thus improve the accuracy of a Deep Learning model when facing completely new data from the problem domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbcbcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Explain the bias-variance tradeoff and how regularization helps in addressing this tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb75c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularization will help select a midpoint between the first scenario of high bias and the later scenario of high variance. This ideal goal of generalization in terms of bias and variance is a low bias and a low variance which is near impossible or difficult to achieve. \n",
    "Hence, the need of the trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb7fac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Describe the concept of L1 and L2 regularization. How do they differ in terms of penalty calculation and their effects on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5e18d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1 Regularization, also called a lasso regression, adds the “absolute value of magnitude” of the coefficient as a penalty term to the loss function. L2 Regularization, also called a ridge regression, adds the “squared magnitude” of the coefficient as the penalty term to the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c6e002",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Discuss the role of regularization in preventing overfitting and improving the generalization of deep \n",
    "learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594e5318",
   "metadata": {},
   "outputs": [],
   "source": [
    "From a practical standpoint, L1 tends to shrink coefficients to zero whereas L2 tends to shrink coefficients evenly. L1 is therefore useful for feature selection, as we can drop any variables associated with coefficients that go to zero. L2, on the other hand, is useful when you have collinear/codependent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610066ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Part 2: Regularization Technique\n",
    "5. Explain Dropout regularization and how it works to reduce overfitting. Discuss the impact of Dropout on \n",
    "model training and inference\n",
    "6. Describe the concept of Early Stopping as a form of regularization. How does it help prevent overfitting \n",
    "during the training process\n",
    "7. Explain the concept of Batch Normalization and its role as a form of regularization. How does Batch \n",
    "Normalization help in preventing overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb52587",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Explain Dropout regularization and how it works to reduce overfitting. Discuss the impact of Dropout on \n",
    "model training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe4e7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dropout is a regularization method approximating concurrent training of many neural networks with various designs. During training, some layer outputs are ignored or dropped at random. This makes the layer appear and is regarded as having a different number of nodes and connectedness to the preceding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43fdebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Describe the concept of Early Stopping as a form of regularization. How does it help prevent overfitting \n",
    "during the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a714e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "In machine learning, early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. \n",
    "Such methods update the learner so as to make it better fit the training data with each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc75490",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Explain the concept of Batch Normalization and its role as a form of regularization. How does Batch \n",
    "Normalization help in preventing overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70f2b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch normalisation has a regularising effect since it adds noise to the inputs of every layer. \n",
    "This discourages overfitting since the model no longer produces deterministic values for a given training example alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748cb4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Part 3: Applying Regularization\n",
    "8. Implement Dropout regularization in a deep learning model using a framework of your choice. Evaluate \n",
    "its impact on model performance and compare it with a model without Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f829182a",
   "metadata": {},
   "source": [
    "## with drop out initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88ab1cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.7359 - accuracy: 0.7927 - val_loss: 0.3457 - val_accuracy: 0.9068\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3231 - accuracy: 0.9061 - val_loss: 0.2717 - val_accuracy: 0.9230\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2687 - accuracy: 0.9217 - val_loss: 0.2408 - val_accuracy: 0.9294\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2359 - accuracy: 0.9317 - val_loss: 0.2106 - val_accuracy: 0.9430\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2118 - accuracy: 0.9395 - val_loss: 0.1944 - val_accuracy: 0.9486\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1917 - accuracy: 0.9445 - val_loss: 0.1741 - val_accuracy: 0.9510\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1752 - accuracy: 0.9490 - val_loss: 0.1650 - val_accuracy: 0.9530\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1609 - accuracy: 0.9539 - val_loss: 0.1538 - val_accuracy: 0.9556\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1490 - accuracy: 0.9572 - val_loss: 0.1431 - val_accuracy: 0.9590\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1386 - accuracy: 0.9603 - val_loss: 0.1383 - val_accuracy: 0.9616\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.735862</td>\n",
       "      <td>0.792727</td>\n",
       "      <td>0.345680</td>\n",
       "      <td>0.9068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.323115</td>\n",
       "      <td>0.906145</td>\n",
       "      <td>0.271669</td>\n",
       "      <td>0.9230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.268744</td>\n",
       "      <td>0.921655</td>\n",
       "      <td>0.240825</td>\n",
       "      <td>0.9294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.235890</td>\n",
       "      <td>0.931673</td>\n",
       "      <td>0.210582</td>\n",
       "      <td>0.9430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.211817</td>\n",
       "      <td>0.939527</td>\n",
       "      <td>0.194420</td>\n",
       "      <td>0.9486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.191677</td>\n",
       "      <td>0.944491</td>\n",
       "      <td>0.174126</td>\n",
       "      <td>0.9510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.175222</td>\n",
       "      <td>0.949000</td>\n",
       "      <td>0.164969</td>\n",
       "      <td>0.9530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.160853</td>\n",
       "      <td>0.953909</td>\n",
       "      <td>0.153826</td>\n",
       "      <td>0.9556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.148955</td>\n",
       "      <td>0.957164</td>\n",
       "      <td>0.143122</td>\n",
       "      <td>0.9590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.138626</td>\n",
       "      <td>0.960255</td>\n",
       "      <td>0.138280</td>\n",
       "      <td>0.9616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  val_loss  val_accuracy\n",
       "0  0.735862  0.792727  0.345680        0.9068\n",
       "1  0.323115  0.906145  0.271669        0.9230\n",
       "2  0.268744  0.921655  0.240825        0.9294\n",
       "3  0.235890  0.931673  0.210582        0.9430\n",
       "4  0.211817  0.939527  0.194420        0.9486\n",
       "5  0.191677  0.944491  0.174126        0.9510\n",
       "6  0.175222  0.949000  0.164969        0.9530\n",
       "7  0.160853  0.953909  0.153826        0.9556\n",
       "8  0.148955  0.957164  0.143122        0.9590\n",
       "9  0.138626  0.960255  0.138280        0.9616"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "mnist=tf.keras.datasets.mnist\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# create a validation data set from the full training data \n",
    "# Scale the data between 0 to 1 by dividing it by 255. as its an unsigned data between 0-255 range\n",
    "X_valid,X_train=X_train_full[:5000]/255., X_train_full[5000:]/255.\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# scale the test set as well\n",
    "X_test = X_test / 255.\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten,Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Creating layers of ANN\n",
    "LAYERS=[tf.keras.layers.Flatten(input_shape=[28, 28], name=\"inputLayer\"),\n",
    "        tf.keras.layers.Dense(300, activation=\"relu\", name=\"hiddenLayer1\",kernel_regularizer=regularizers.L2(1.0e-04)),\n",
    "        tf.keras.layers.Dropout(rate=0.2),\n",
    "        tf.keras.layers.Dense(100, activation=\"relu\", name=\"hiddenLayer2\",kernel_regularizer=regularizers.L1L2(l1=1.0e-05,l2=1.0e-04)),\n",
    "        tf.keras.layers.Dropout(rate=0.2),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\", name=\"outputLayer\")]\n",
    "\n",
    "model_clf=tf.keras.models.Sequential(LAYERS)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten,Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "\n",
    "model_clf=Sequential()\n",
    "model_clf.add(Flatten(input_shape=[28, 28], name=\"inputLayer\")),\n",
    "model_clf.add(Dense(64, activation=\"relu\", name=\"hiddenLayer1\")),\n",
    "model_clf.add(Dense(32, activation=\"relu\", name=\"hiddenLayer2\")),\n",
    "model_clf.add(Dense(10, activation=\"softmax\", name=\"outputLayer\"))\n",
    "\n",
    "LOSS_FUNCTION = \"sparse_categorical_crossentropy\" # use => tf.losses.sparse_categorical_crossentropy\n",
    "OPTIMIZER = \"SGD\" # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)\n",
    "METRICS = [\"accuracy\"]\n",
    "\n",
    "\n",
    "model_clf.compile(loss=LOSS_FUNCTION,\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=METRICS)\n",
    "\n",
    "EPOCHS = 10\n",
    "VALIDATION_SET = (X_valid, y_valid)\n",
    "\n",
    "history=model_clf.fit(X_train,y_train,epochs=EPOCHS,validation_data=VALIDATION_SET,batch_size=32)\n",
    "\n",
    "\n",
    "pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fccb02",
   "metadata": {},
   "source": [
    "## without drop out initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd359da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.7701 - accuracy: 0.7867 - val_loss: 0.3486 - val_accuracy: 0.9040\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3255 - accuracy: 0.9063 - val_loss: 0.2759 - val_accuracy: 0.9228\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2701 - accuracy: 0.9231 - val_loss: 0.2356 - val_accuracy: 0.9322\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2360 - accuracy: 0.9324 - val_loss: 0.2125 - val_accuracy: 0.9390\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2103 - accuracy: 0.9396 - val_loss: 0.1881 - val_accuracy: 0.9474\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1896 - accuracy: 0.9459 - val_loss: 0.1754 - val_accuracy: 0.9510\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1726 - accuracy: 0.9505 - val_loss: 0.1603 - val_accuracy: 0.9538\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1583 - accuracy: 0.9536 - val_loss: 0.1495 - val_accuracy: 0.9578\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1464 - accuracy: 0.9576 - val_loss: 0.1403 - val_accuracy: 0.9592\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1350 - accuracy: 0.9609 - val_loss: 0.1337 - val_accuracy: 0.9616\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.770140</td>\n",
       "      <td>0.786745</td>\n",
       "      <td>0.348629</td>\n",
       "      <td>0.9040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.325456</td>\n",
       "      <td>0.906309</td>\n",
       "      <td>0.275939</td>\n",
       "      <td>0.9228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.270105</td>\n",
       "      <td>0.923091</td>\n",
       "      <td>0.235634</td>\n",
       "      <td>0.9322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.236014</td>\n",
       "      <td>0.932382</td>\n",
       "      <td>0.212473</td>\n",
       "      <td>0.9390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.210346</td>\n",
       "      <td>0.939600</td>\n",
       "      <td>0.188120</td>\n",
       "      <td>0.9474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.189596</td>\n",
       "      <td>0.945891</td>\n",
       "      <td>0.175366</td>\n",
       "      <td>0.9510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.172632</td>\n",
       "      <td>0.950527</td>\n",
       "      <td>0.160347</td>\n",
       "      <td>0.9538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.158313</td>\n",
       "      <td>0.953582</td>\n",
       "      <td>0.149483</td>\n",
       "      <td>0.9578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.146422</td>\n",
       "      <td>0.957564</td>\n",
       "      <td>0.140260</td>\n",
       "      <td>0.9592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.135000</td>\n",
       "      <td>0.960891</td>\n",
       "      <td>0.133662</td>\n",
       "      <td>0.9616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  val_loss  val_accuracy\n",
       "0  0.770140  0.786745  0.348629        0.9040\n",
       "1  0.325456  0.906309  0.275939        0.9228\n",
       "2  0.270105  0.923091  0.235634        0.9322\n",
       "3  0.236014  0.932382  0.212473        0.9390\n",
       "4  0.210346  0.939600  0.188120        0.9474\n",
       "5  0.189596  0.945891  0.175366        0.9510\n",
       "6  0.172632  0.950527  0.160347        0.9538\n",
       "7  0.158313  0.953582  0.149483        0.9578\n",
       "8  0.146422  0.957564  0.140260        0.9592\n",
       "9  0.135000  0.960891  0.133662        0.9616"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "mnist=tf.keras.datasets.mnist\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# create a validation data set from the full training data \n",
    "# Scale the data between 0 to 1 by dividing it by 255. as its an unsigned data between 0-255 range\n",
    "X_valid,X_train=X_train_full[:5000]/255., X_train_full[5000:]/255.\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# scale the test set as well\n",
    "X_test = X_test / 255.\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten,Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Creating layers of ANN\n",
    "LAYERS=[tf.keras.layers.Flatten(input_shape=[28, 28], name=\"inputLayer\"),\n",
    "        tf.keras.layers.Dense(300, activation=\"relu\", name=\"hiddenLayer1\",kernel_regularizer=regularizers.L2(1.0e-04)),\n",
    "        tf.keras.layers.Dense(100, activation=\"relu\", name=\"hiddenLayer2\",kernel_regularizer=regularizers.L1L2(l1=1.0e-05,l2=1.0e-04)),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\", name=\"outputLayer\")]\n",
    "\n",
    "model_clf=tf.keras.models.Sequential(LAYERS)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten,Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "\n",
    "model_clf=Sequential()\n",
    "model_clf.add(Flatten(input_shape=[28, 28], name=\"inputLayer\")),\n",
    "model_clf.add(Dense(64, activation=\"relu\", name=\"hiddenLayer1\")),\n",
    "model_clf.add(Dense(32, activation=\"relu\", name=\"hiddenLayer2\")),\n",
    "model_clf.add(Dense(10, activation=\"softmax\", name=\"outputLayer\"))\n",
    "\n",
    "LOSS_FUNCTION = \"sparse_categorical_crossentropy\" # use => tf.losses.sparse_categorical_crossentropy\n",
    "OPTIMIZER = \"SGD\" # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)\n",
    "METRICS = [\"accuracy\"]\n",
    "\n",
    "\n",
    "model_clf.compile(loss=LOSS_FUNCTION,\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=METRICS)\n",
    "\n",
    "EPOCHS = 10\n",
    "VALIDATION_SET = (X_valid, y_valid)\n",
    "\n",
    "history=model_clf.fit(X_train,y_train,epochs=EPOCHS,validation_data=VALIDATION_SET,batch_size=32)\n",
    "\n",
    "\n",
    "pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f7aef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a920ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Discuss the considerations and tradeoffs when choosing the appropriate regularization technique for a \n",
    "given deep learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797c9330",
   "metadata": {},
   "source": [
    "## if our aim is to reduce overfit, we use regularization L1 Regularization and if our aim is to feature reduction, we use L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb66af74",
   "metadata": {},
   "source": [
    "## in above example, dropout is not significantly effecting accuracy/loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
