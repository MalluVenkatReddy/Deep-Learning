{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cc7806",
   "metadata": {},
   "outputs": [],
   "source": [
    "Part 1: Understanding Weight Initialization\n",
    "1. Explain the importance of weight initialization in artificial neural networks. Why is it necessary to initialize \n",
    "the weights carefully\n",
    "2. Describe the challenges associated with improper weight initialization. How do these issues affect model \n",
    "training and convergence\n",
    "3. Discuss the concept of variance and how it relates to weight initialization. Why is it crucial to consider the \n",
    "variance of weights during initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4055ec5",
   "metadata": {},
   "source": [
    "## 1. importance of weight initialization in artificial neural networks. Why is it necessary to initialize the weights carefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a21d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "The aim of weight initialization is to prevent layer activation outputs from exploding or vanishing during the course of a forward pass through a deep neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4fee9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Its main objective is to prevent layer activation outputs from exploding or vanishing gradients during the forward propagation. \n",
    "If either of the problems occurs, loss gradients will either be too large or too small, and the network will take more time to converge if it is even able to do so at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88c5c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "The weights of artificial neural networks must be initialized to small random numbers. \n",
    "This is because this is an expectation of the stochastic optimization algorithm used to train the model, called stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b2a53e",
   "metadata": {},
   "source": [
    "## 2.challenges associated with improper weight initialization. How do these issues affect model training and convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86bb86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "If all the weights are initialized with 0, the derivative with respect to loss function is the same for every w in W[l], thus all weights have the same value in subsequent iterations. \n",
    "This makes hidden units symmetric and continues for all the n iterations i.e. setting weights to 0 does not make it better than a linear model. \n",
    "An important thing to keep in mind is that biases have no effect what so ever when initialized with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaf5384",
   "metadata": {},
   "outputs": [],
   "source": [
    "Studies have shown that initializing the weights with values sampled from a random distribution instead of constant values like zeros and ones actually helps a neural net train better and faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d52965",
   "metadata": {},
   "source": [
    "## 3. Discuss the concept of variance and how it relates to weight initialization. Why is it crucial to consider the variance of weights during initialization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d1ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "The term variance refers to a statistical measurement of the spread between numbers in a data set. \n",
    "More specifically, variance measures how far each number in the set is from the mean (average), and thus from every other number in the set. \n",
    "Variance is often depicted by this symbol: σ2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863866f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Variance of forward flowing signal for each and every hidden layer is equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4ddc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "Part 2: Weight Initialization Technique\n",
    "4. Explain the concept of zero initialization. Discuss its potential limitations and when it can be appropriate \n",
    "to use\n",
    "5. Describe the process of random initialization. How can random initialization be adjusted to mitigate \n",
    "potential issues like saturation or vanishing/exploding gradients\n",
    "6. Discuss the concept of Xavier/Glorot initialization. Explain how it addresses the challenges of improper \n",
    "weight initialization and the underlying theory behind it\n",
    "7. Explain the concept of He initialization. How does it differ from Xavier initialization, and when is it \n",
    "preferred?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b220ba20",
   "metadata": {},
   "source": [
    "## 4. Explain the concept of zero initialization. Discuss its potential limitations and when it can be appropriate to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a8e7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Initializing all the weights with zeros leads the neurons to learn the same features during training. \n",
    "In fact, any constant initialization scheme will perform very poorly. Consider a neural network with two hidden units, and assume we initialize all the biases to 0 and the weights with some constant α."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8871da",
   "metadata": {},
   "outputs": [],
   "source": [
    "If all the weights are initialized to zeros, the derivatives will remain same for every w in W[l]. \n",
    "As a result, neurons will learn same features in each iterations. This problem is known as network failing to break symmetry. \n",
    "And not only zero, any constant initialization will produce a poor result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2852d3",
   "metadata": {},
   "source": [
    "## 5. Describe the process of random initialization. How can random initialization be adjusted to mitigate potential issues like saturation or vanishing/exploding gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ea128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random Initialization for neural networks aids in the symmetry-breaking process and improves accuracy. \n",
    "The weights are randomly initialized in this manner, very close to zero. \n",
    "As a result, symmetry is broken, and each neuron no longer performs the same computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64220868",
   "metadata": {},
   "source": [
    "## 6. Discuss the concept of Xavier/Glorot initialization. Explain how it addresses the challenges of improper weight initialization and the underlying theory behind it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6427353",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xavier Glorot's initialization is one of the most widely used methods for initializing weight matrices in neural networks. \n",
    "While in practice, it is straightforward to utilize in your deep learning setup, reflecting upon the mathematical reasoning behind this standard initialization technique can prove most beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186a142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xavier/Glorot Initialization is used to maintain the same smooth distribution for both the forward pass as well the backpropagation. But, Glorot Initialization fails for ReLU, instead we use He Initialization for ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be519b4a",
   "metadata": {},
   "source": [
    "## 7. Explain the concept of He initialization. How does it differ from Xavier initialization, and when is it preferred?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a7a310",
   "metadata": {},
   "outputs": [],
   "source": [
    "He initialization works better for layers with ReLu activation. \n",
    "Xavier initialization works better for layers with sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42119029",
   "metadata": {},
   "outputs": [],
   "source": [
    "He Initialization, is an initialization method for neural networks that takes into account the non-linearity of activation functions, such as ReLU activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4239f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "Part 3: Applying Weight Initialization\n",
    "8. Implement different weight initialization techniques (zero initialization, random initialization, Xavier \n",
    "initialization, and He initialization) in a neural network using a framework of your choice. Train the model \n",
    "on a suitable dataset and compare the performance of the initialized models\n",
    "9. Discuss the considerations and tradeoffs when choosing the appropriate weight initialization technique \n",
    "for a given neural network architecture and task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39cdf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Implement different weight initialization techniques (zero initialization, random initialization, Xavier \n",
    "initialization, and He initialization) in a neural network using a framework of your choice. Train the model \n",
    "on a suitable dataset and compare the performance of the initialized models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becd8239",
   "metadata": {},
   "source": [
    "## With weight initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "858d770e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.7122 - accuracy: 0.8010 - val_loss: 0.3362 - val_accuracy: 0.9068\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3160 - accuracy: 0.9102 - val_loss: 0.2646 - val_accuracy: 0.9238\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2635 - accuracy: 0.9240 - val_loss: 0.2281 - val_accuracy: 0.9342\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2299 - accuracy: 0.9335 - val_loss: 0.2025 - val_accuracy: 0.9420\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2062 - accuracy: 0.9404 - val_loss: 0.1944 - val_accuracy: 0.9458\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1873 - accuracy: 0.9462 - val_loss: 0.1719 - val_accuracy: 0.9528\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1713 - accuracy: 0.9508 - val_loss: 0.1594 - val_accuracy: 0.9572\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1584 - accuracy: 0.9543 - val_loss: 0.1597 - val_accuracy: 0.9566\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1470 - accuracy: 0.9584 - val_loss: 0.1446 - val_accuracy: 0.9608\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1375 - accuracy: 0.9608 - val_loss: 0.1381 - val_accuracy: 0.9632\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.712177</td>\n",
       "      <td>0.801018</td>\n",
       "      <td>0.336159</td>\n",
       "      <td>0.9068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.315951</td>\n",
       "      <td>0.910218</td>\n",
       "      <td>0.264639</td>\n",
       "      <td>0.9238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.263491</td>\n",
       "      <td>0.923982</td>\n",
       "      <td>0.228060</td>\n",
       "      <td>0.9342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.229933</td>\n",
       "      <td>0.933527</td>\n",
       "      <td>0.202457</td>\n",
       "      <td>0.9420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.206207</td>\n",
       "      <td>0.940364</td>\n",
       "      <td>0.194384</td>\n",
       "      <td>0.9458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.187263</td>\n",
       "      <td>0.946200</td>\n",
       "      <td>0.171880</td>\n",
       "      <td>0.9528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.171345</td>\n",
       "      <td>0.950836</td>\n",
       "      <td>0.159354</td>\n",
       "      <td>0.9572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.158387</td>\n",
       "      <td>0.954345</td>\n",
       "      <td>0.159702</td>\n",
       "      <td>0.9566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.147018</td>\n",
       "      <td>0.958364</td>\n",
       "      <td>0.144624</td>\n",
       "      <td>0.9608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.137526</td>\n",
       "      <td>0.960836</td>\n",
       "      <td>0.138137</td>\n",
       "      <td>0.9632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  val_loss  val_accuracy\n",
       "0  0.712177  0.801018  0.336159        0.9068\n",
       "1  0.315951  0.910218  0.264639        0.9238\n",
       "2  0.263491  0.923982  0.228060        0.9342\n",
       "3  0.229933  0.933527  0.202457        0.9420\n",
       "4  0.206207  0.940364  0.194384        0.9458\n",
       "5  0.187263  0.946200  0.171880        0.9528\n",
       "6  0.171345  0.950836  0.159354        0.9572\n",
       "7  0.158387  0.954345  0.159702        0.9566\n",
       "8  0.147018  0.958364  0.144624        0.9608\n",
       "9  0.137526  0.960836  0.138137        0.9632"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "mnist=tf.keras.datasets.mnist\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# create a validation data set from the full training data \n",
    "# Scale the data between 0 to 1 by dividing it by 255. as its an unsigned data between 0-255 range\n",
    "X_valid,X_train=X_train_full[:5000]/255., X_train_full[5000:]/255.\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# scale the test set as well\n",
    "X_test = X_test / 255.\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten,Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Creating layers of ANN\n",
    "LAYERS=[tf.keras.layers.Flatten(input_shape=[28, 28], name=\"inputLayer\"),\n",
    "        tf.keras.layers.Dense(300, activation=\"relu\", name=\"hiddenLayer1\",kernel_regularizer=regularizers.L2(1.0e-04)),\n",
    "        tf.keras.layers.Dense(100, activation=\"relu\", name=\"hiddenLayer2\",kernel_regularizer=regularizers.L1L2(l1=1.0e-05,l2=1.0e-04)),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\", name=\"outputLayer\")]\n",
    "\n",
    "model_clf=tf.keras.models.Sequential(LAYERS)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten,Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "\n",
    "model_clf=Sequential()\n",
    "model_clf.add(Flatten(input_shape=[28, 28], name=\"inputLayer\")),\n",
    "model_clf.add(Dense(64, activation=\"relu\", name=\"hiddenLayer1\",kernel_initializer=tf.keras.initializers.HeNormal(seed=None))),\n",
    "model_clf.add(Dense(32, activation=\"relu\", name=\"hiddenLayer2\",kernel_initializer=tf.keras.initializers.HeNormal(seed=None))),\n",
    "model_clf.add(Dense(10, activation=\"softmax\", name=\"outputLayer\"))\n",
    "\n",
    "LOSS_FUNCTION = \"sparse_categorical_crossentropy\" # use => tf.losses.sparse_categorical_crossentropy\n",
    "OPTIMIZER = \"SGD\" # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)\n",
    "METRICS = [\"accuracy\"]\n",
    "\n",
    "\n",
    "model_clf.compile(loss=LOSS_FUNCTION,\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=METRICS)\n",
    "\n",
    "EPOCHS = 10\n",
    "VALIDATION_SET = (X_valid, y_valid)\n",
    "\n",
    "history=model_clf.fit(X_train,y_train,epochs=EPOCHS,validation_data=VALIDATION_SET,batch_size=32)\n",
    "\n",
    "\n",
    "pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6e26c3",
   "metadata": {},
   "source": [
    "## Without weight initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce38c9b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.7910 - accuracy: 0.7749 - val_loss: 0.3512 - val_accuracy: 0.9050\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3324 - accuracy: 0.9048 - val_loss: 0.2818 - val_accuracy: 0.9218\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2786 - accuracy: 0.9202 - val_loss: 0.2415 - val_accuracy: 0.9344\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2447 - accuracy: 0.9295 - val_loss: 0.2128 - val_accuracy: 0.9406\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2186 - accuracy: 0.9380 - val_loss: 0.1956 - val_accuracy: 0.9448\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1976 - accuracy: 0.9430 - val_loss: 0.1803 - val_accuracy: 0.9496\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1800 - accuracy: 0.9483 - val_loss: 0.1650 - val_accuracy: 0.9554\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1648 - accuracy: 0.9525 - val_loss: 0.1533 - val_accuracy: 0.9610\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1521 - accuracy: 0.9568 - val_loss: 0.1452 - val_accuracy: 0.9608\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1409 - accuracy: 0.9589 - val_loss: 0.1385 - val_accuracy: 0.9616\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.790990</td>\n",
       "      <td>0.774909</td>\n",
       "      <td>0.351218</td>\n",
       "      <td>0.9050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.332397</td>\n",
       "      <td>0.904800</td>\n",
       "      <td>0.281841</td>\n",
       "      <td>0.9218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.278637</td>\n",
       "      <td>0.920236</td>\n",
       "      <td>0.241497</td>\n",
       "      <td>0.9344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.244738</td>\n",
       "      <td>0.929545</td>\n",
       "      <td>0.212757</td>\n",
       "      <td>0.9406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.218616</td>\n",
       "      <td>0.938036</td>\n",
       "      <td>0.195551</td>\n",
       "      <td>0.9448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.197561</td>\n",
       "      <td>0.943018</td>\n",
       "      <td>0.180302</td>\n",
       "      <td>0.9496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.179976</td>\n",
       "      <td>0.948255</td>\n",
       "      <td>0.165023</td>\n",
       "      <td>0.9554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.164794</td>\n",
       "      <td>0.952473</td>\n",
       "      <td>0.153268</td>\n",
       "      <td>0.9610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.152120</td>\n",
       "      <td>0.956800</td>\n",
       "      <td>0.145173</td>\n",
       "      <td>0.9608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.140851</td>\n",
       "      <td>0.958927</td>\n",
       "      <td>0.138519</td>\n",
       "      <td>0.9616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  val_loss  val_accuracy\n",
       "0  0.790990  0.774909  0.351218        0.9050\n",
       "1  0.332397  0.904800  0.281841        0.9218\n",
       "2  0.278637  0.920236  0.241497        0.9344\n",
       "3  0.244738  0.929545  0.212757        0.9406\n",
       "4  0.218616  0.938036  0.195551        0.9448\n",
       "5  0.197561  0.943018  0.180302        0.9496\n",
       "6  0.179976  0.948255  0.165023        0.9554\n",
       "7  0.164794  0.952473  0.153268        0.9610\n",
       "8  0.152120  0.956800  0.145173        0.9608\n",
       "9  0.140851  0.958927  0.138519        0.9616"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "mnist=tf.keras.datasets.mnist\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# create a validation data set from the full training data \n",
    "# Scale the data between 0 to 1 by dividing it by 255. as its an unsigned data between 0-255 range\n",
    "X_valid,X_train=X_train_full[:5000]/255., X_train_full[5000:]/255.\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# scale the test set as well\n",
    "X_test = X_test / 255.\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten,Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Creating layers of ANN\n",
    "LAYERS=[tf.keras.layers.Flatten(input_shape=[28, 28], name=\"inputLayer\"),\n",
    "        tf.keras.layers.Dense(300, activation=\"relu\", name=\"hiddenLayer1\",kernel_regularizer=regularizers.L2(1.0e-04)),\n",
    "        tf.keras.layers.Dense(100, activation=\"relu\", name=\"hiddenLayer2\",kernel_regularizer=regularizers.L1L2(l1=1.0e-05,l2=1.0e-04)),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\", name=\"outputLayer\")]\n",
    "\n",
    "model_clf=tf.keras.models.Sequential(LAYERS)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten,Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "\n",
    "model_clf=Sequential()\n",
    "model_clf.add(Flatten(input_shape=[28, 28], name=\"inputLayer\")),\n",
    "model_clf.add(Dense(64, activation=\"relu\", name=\"hiddenLayer1\")),\n",
    "model_clf.add(Dense(32, activation=\"relu\", name=\"hiddenLayer2\")),\n",
    "model_clf.add(Dense(10, activation=\"softmax\", name=\"outputLayer\"))\n",
    "\n",
    "LOSS_FUNCTION = \"sparse_categorical_crossentropy\" # use => tf.losses.sparse_categorical_crossentropy\n",
    "OPTIMIZER = \"SGD\" # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)\n",
    "METRICS = [\"accuracy\"]\n",
    "\n",
    "\n",
    "model_clf.compile(loss=LOSS_FUNCTION,\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=METRICS)\n",
    "\n",
    "EPOCHS = 10\n",
    "VALIDATION_SET = (X_valid, y_valid)\n",
    "\n",
    "history=model_clf.fit(X_train,y_train,epochs=EPOCHS,validation_data=VALIDATION_SET,batch_size=32)\n",
    "\n",
    "\n",
    "pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d922cee6",
   "metadata": {},
   "source": [
    "## with weight initialization, accuracy is improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf080d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Discuss the considerations and tradeoffs when choosing the appropriate weight initialization technique \n",
    "for a given neural network architecture and task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1115769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tanh, we use Xavier/Glorot normal initialization\n",
    "for sigmoid/reLU, we use He initialization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
